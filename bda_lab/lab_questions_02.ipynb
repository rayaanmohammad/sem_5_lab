{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit, when, sum as sum_, avg, explode, split, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lplab/anaconda3/lib/python3.7/site-packages/pyspark/context.py:317: FutureWarning: Python 3.7 support is deprecated in Spark 3.4.\n",
      "  warnings.warn(\"Python 3.7 support is deprecated in Spark 3.4.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      "+----+---+------+------+\n",
      "|Name|Age|Gender|Salary|\n",
      "+----+---+------+------+\n",
      "|John| 30|  Male|  1000|\n",
      "|Jane| 25|Female|  1500|\n",
      "| Sam| 28|  Male|  1200|\n",
      "|Anna| 22|Female|   800|\n",
      "|Mike| 35|  Male|  2000|\n",
      "+----+---+------+------+\n",
      "\n",
      "Transformed DataFrame:\n",
      "+----+---+------+------+---------+\n",
      "|Name|Age|Gender|Salary|Age_Group|\n",
      "+----+---+------+------+---------+\n",
      "|John| 30|  Male|  1000|    Adult|\n",
      "| Sam| 28|  Male|  1200|    Adult|\n",
      "|Mike| 35|  Male|  2000|   Senior|\n",
      "+----+---+------+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Question 1: Implement a PySpark script that applies transformations like filter and withColumn on a Dataframe.\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Lab 02 Questions\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "data = [\n",
    "    (\"John\", 30, \"Male\", 1000),\n",
    "    (\"Jane\", 25, \"Female\", 1500),\n",
    "    (\"Sam\", 28, \"Male\", 1200),\n",
    "    (\"Anna\", 22, \"Female\", 800),\n",
    "    (\"Mike\", 35, \"Male\", 2000)\n",
    "]\n",
    "\n",
    "columns = [\"Name\", \"Age\", \"Gender\",\"Salary\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "print(\"Original DataFrame:\")\n",
    "df.show()\n",
    "\n",
    "#filtering based on age\n",
    "filtered_df = df.filter(col(\"Age\") > 25)\n",
    "\n",
    "#adding column with the function withColumn\n",
    "transformed_df = filtered_df.withColumn(\n",
    "    \"Age_Group\",\n",
    "    when(col(\"Age\") > 30, lit(\"Senior\")).otherwise(lit(\"Adult\"))\n",
    ")\n",
    "\n",
    "print(\"Transformed DataFrame:\")\n",
    "transformed_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Showing DataFrame:\n",
      "+----+---+------+------+\n",
      "|Name|Age|Gender|Salary|\n",
      "+----+---+------+------+\n",
      "|John| 30|  Male|  1000|\n",
      "|Jane| 25|Female|  1500|\n",
      "| Sam| 28|  Male|  1200|\n",
      "|Anna| 22|Female|   800|\n",
      "|Mike| 35|  Male|  2000|\n",
      "+----+---+------+------+\n",
      "\n",
      "\n",
      "Number of rows in DataFrame: 5\n"
     ]
    }
   ],
   "source": [
    "#Question 2: Write a PySpark script that performs actions like count and show on a DataFrame.\n",
    "\n",
    "# Show the DataFrame\n",
    "print(\"Showing DataFrame:\")\n",
    "df.show()\n",
    "\n",
    "# Count the number of rows in the DataFrame\n",
    "row_count = df.count()\n",
    "print(f\"\\nNumber of rows in DataFrame: {row_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Salary: 6500\n",
      "Average Salary: 1300.0\n"
     ]
    }
   ],
   "source": [
    "#Question 3: Demonstrate how to perform basic aggregations (e.g., sum, average) on a PySpark DataFrame.\n",
    "\n",
    "# Calculate the sum of the 'Salary' column\n",
    "total_salary = df.agg(sum_(\"Salary\").alias(\"Total_Salary\")).collect()[0][\"Total_Salary\"]\n",
    "print(f\"Total Salary: {total_salary}\")\n",
    "\n",
    "# Calculate the average of the 'Salary' column\n",
    "average_salary = df.agg(avg(\"Salary\").alias(\"Average_Salary\")).collect()[0][\"Average_Salary\"]\n",
    "print(f\"Average Salary: {average_salary}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame written to output_data.csv\n",
      "\n",
      "DataFrame written to output_data_single.csv\n"
     ]
    }
   ],
   "source": [
    "#Question 4: Show how to write a PySpark DataFrame to a CSV file.\n",
    "\n",
    "# Define the path to the CSV file\n",
    "output_path = \"output_data.csv\"\n",
    "\n",
    "# Write the DataFrame to a CSV file (writes it in different files per row)\n",
    "df.write \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(output_path)\n",
    "\n",
    "print(f\"\\nDataFrame written to {output_path}\")\n",
    "\n",
    "#You can use .coalesce() to write to one file\n",
    "output_path = \"output_data_single.csv\"\n",
    "df.coalesce(1).write \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(output_path)\n",
    "\n",
    "print(f\"\\nDataFrame written to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      "+------------------------+\n",
      "|value                   |\n",
      "+------------------------+\n",
      "|Hello World             |\n",
      "|Hello pyspark           |\n",
      "|Welcome to pyspark World|\n",
      "+------------------------+\n",
      "\n",
      "\n",
      "Word Count Results:\n",
      "+-------+-----+\n",
      "|word   |count|\n",
      "+-------+-----+\n",
      "|World  |2    |\n",
      "|Hello  |2    |\n",
      "|pyspark|2    |\n",
      "|Welcome|1    |\n",
      "|to     |1    |\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Question 5: Implement wordcount program in PySpark.\n",
    "input_file_path = \"input_text.txt\"  # Replace with the path to your input text file\n",
    "\n",
    "df = spark.read.text(input_file_path)\n",
    "\n",
    "print(\"Original DataFrame:\")\n",
    "df.show(truncate=False)\n",
    "\n",
    "# Perform WordCount\n",
    "# Split each line into words, explode to create a row for each word, and count the occurrences\n",
    "word_counts = df.select(\n",
    "    explode(\n",
    "        split(col(\"value\"), r\"\\s+\")\n",
    "    ).alias(\"word\")\n",
    ").groupBy(\"word\").count()\n",
    "\n",
    "# Show the WordCount results\n",
    "print(\"\\nWord Count Results:\")\n",
    "word_counts.show(truncate=False)\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
